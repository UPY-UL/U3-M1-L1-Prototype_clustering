{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "**IMPORTANT: DO NOT COPY OR SPLIT CELLS.** If you do, you'll mess the autograder. If need more cells to work or test things out, create a new cell. You may add as many new cells as you need.\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and group below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COURSE = \"Unsupervised Learning 2021\"\n",
    "GROUP = \"D8A\"\n",
    "NAME = \"\" # Match your GitHub Classroom ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means and K-medoids Clustering \n",
    "\n",
    "## K-means\n",
    "\n",
    "In this exercise, you will implement the K-means clustering algorithm and apply it to compress an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scientific and vector computation for python\n",
    "import numpy as np\n",
    "\n",
    "# Plotting library\n",
    "import matplotlib.pyplot as plt\n",
    "#from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib as mpl\n",
    "\n",
    "from IPython.display import HTML, display, clear_output\n",
    "try:\n",
    "    plt.rcParams[\"animation.html\"] = \"jshtml\"\n",
    "except ValueError:\n",
    "    plt.rcParams[\"animation.html\"] = \"html5\"\n",
    "    \n",
    "# will be used to load MATLAB mat datafile format\n",
    "from scipy.io import loadmat\n",
    "\n",
    "# library written for this exercise providing additional functions\n",
    "import utils\n",
    "\n",
    "# tells matplotlib to embed plots within the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing K-means\n",
    "\n",
    "The K-means algorithm is as follows:\n",
    "\n",
    "```python\n",
    "centroids = kMeansInitCentroids(X, K)\n",
    "for i in range(iterations):\n",
    "    # Cluster assignment step: Assign each data point to the\n",
    "    # closest centroid. idx[i] corresponds to cˆ(i), the index\n",
    "    # of the centroid assigned to example i\n",
    "    idx = findClosestCentroids(X, centroids)\n",
    "    \n",
    "    # Move centroid step: Compute means based on centroid\n",
    "    # assignments\n",
    "    centroids = computeMeans(X, idx, K)\n",
    "```\n",
    "\n",
    "The inner-loop of the algorithm repeatedly carries out two steps: (1) Assigning each training example $x^{(i)}$ to its closest centroid, and (2) Recomputing the mean of each centroid using the points assigned to it.You will implement the two phases of the K-means algorithm separately\n",
    "in the next sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding closest centroids\n",
    "\n",
    "In the “cluster assignment” phase of the K-means algorithm, the algorithm assigns every training example $x^{(i)}$ to its closest centroid, given the current positions of centroids. Specifically, for every example $i$ we set\n",
    "\n",
    "$$c^{(i)} := j \\quad \\text{that minimizes} \\quad \\lvert\\rvert x^{(i)} - \\mu_j  \\lvert\\rvert^2, $$\n",
    "\n",
    "where $c^{(i)}$ is the index of the centroid that is closest to $x^{(i)}$, and $\\mu_j$ is the position (value) of the $j^{th}$ centroid. Note that $c^{(i)}$ corresponds to `idx[i]` in the starter code.\n",
    "\n",
    "Your task is to complete the code in the function `findClosestCentroids`. This function takes the data matrix `X` and the locations of all centroids inside `centroids` and should output a one-dimensional array `idx` that holds the index (a value in $\\{1, ..., K\\}$, where $K$ is total number of centroids) of the closest centroid to every training example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "24e74d0c4afbfd2bf6867d9ced065c20",
     "grade": false,
     "grade_id": "cell-895fe28a1971ae2a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def findClosestCentroids(X, centroids):\n",
    "    \"\"\"\n",
    "    Computes the centroid memberships for every example.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array_like\n",
    "        The dataset of size (m, n) where each row is a single example. \n",
    "        That is, we have m examples each of n dimensions.\n",
    "        \n",
    "    centroids : array_like\n",
    "        The k-means centroids of size (K, n). K is the number\n",
    "        of clusters, and n is the the data dimension.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    idx : array_like\n",
    "        A vector of size (m, ) which holds the centroids assignment for each\n",
    "        example (row) in the dataset X.\n",
    "    \"\"\"\n",
    "    # Set K\n",
    "    K = centroids.shape[0]\n",
    "\n",
    "    # You need to return the following variables correctly.\n",
    "    idx = np.zeros(X.shape[0], dtype=int)\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7f7ca30e80630de5ce0172c3d4110ff1",
     "grade": false,
     "grade_id": "cell-8bdce157b4979221",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Once you have completed the code in `findClosestCentroids`, the following cell will run your code and you should see the output `[0 2 1]` corresponding to the centroid assignments for the first 3 examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6e99ae8249ba2767afbd1263f49f0d5f",
     "grade": true,
     "grade_id": "cell-121f1b7b6f644442",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Load an example dataset that we will be using\n",
    "data = loadmat('Data/ex7data2.mat')\n",
    "X = data['X']\n",
    "\n",
    "# Select an initial set of centroids\n",
    "K = 3   # 3 Centroids\n",
    "initial_centroids = np.array([[3, 3], [6, 2], [8, 5]])\n",
    "\n",
    "# Find the closest centroids for the examples using the initial_centroids\n",
    "idx = findClosestCentroids(X, initial_centroids)\n",
    "\n",
    "print('Closest centroids for the first 3 examples:')\n",
    "print(idx[:3])\n",
    "print('(the closest centroids should be 0, 2, 1 respectively)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing centroid means\n",
    "\n",
    "Given assignments of every point to a centroid, the second phase of the algorithm recomputes, for each centroid, the mean of the points that were assigned to it. Specifically, for every centroid $k$ we set\n",
    "\n",
    "$$ \\mu_k := \\frac{1}{\\left| C_k\\right|} \\sum_{i \\in C_k} x^{(i)}$$\n",
    "\n",
    "where $C_k$ is the set of examples that are assigned to centroid $k$. Concretely, if two examples say $x^{(3)}$ and $x^{(5)}$ are assigned to centroid $k = 2$, then you should update $\\mu_2 = \\frac{1}{2} \\left( x^{(3)} + x^{(5)} \\right)$.\n",
    "\n",
    "You should now complete the code in the function `computeCentroids`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f2624703c3eb57515b56e48e3af8710c",
     "grade": false,
     "grade_id": "cell-fa3673ae41682343",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def computeCentroids(X, idx, K):\n",
    "    \"\"\"\n",
    "    Returns the new centroids by computing the means of the data points\n",
    "    assigned to each centroid.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array_like\n",
    "        The datset where each row is a single data point. That is, it \n",
    "        is a matrix of size (m, n) where there are m datapoints each\n",
    "        having n dimensions. \n",
    "    \n",
    "    idx : array_like \n",
    "        A vector (size m) of centroid assignments (i.e. each entry in range [0 ... K-1])\n",
    "        for each example.\n",
    "    \n",
    "    K : int\n",
    "        Number of clusters\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    centroids : array_like\n",
    "        A matrix of size (K, n) where each row is the mean of the data \n",
    "        points assigned to it.\n",
    "    \"\"\"\n",
    "    # Useful variables\n",
    "    m, n = X.shape\n",
    "    # You need to return the following variables correctly.\n",
    "    centroids = np.zeros((K, n))\n",
    "\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3f8971c15d1b1d6d3abd43696e61a097",
     "grade": false,
     "grade_id": "cell-c4b5b7524d313380",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Once you have completed the code in `computeCentroids`, the following cell will run your code and output the centroids after the first step of K-means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1da08ef42ef86211e2b03e200ec3aba1",
     "grade": true,
     "grade_id": "cell-7b5c83573753b431",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Compute means based on the closest centroids found in the previous part.\n",
    "centroids = computeCentroids(X, idx, K)\n",
    "\n",
    "print('Centroids computed after initial finding of closest centroids:')\n",
    "print(centroids)\n",
    "print('\\nThe centroids should be')\n",
    "print('   [ 2.428301 3.157924 ]')\n",
    "print('   [ 5.813503 2.633656 ]')\n",
    "print('   [ 7.119387 3.616684 ]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means on example dataset \n",
    "\n",
    "After you have completed the two functions (`findClosestCentroids` and `computeCentroids`), you have all the necessary pieces to run the K-means algorithm. The next cell  will run the K-means algorithm on a toy 2D dataset to help you understand how K-means works. Your functions are called from inside the `runKmeans` function (in this assignment's `utils.py` module). We encourage you to take a look at the function to understand how it works. Notice that the code calls the two functions you implemented in a loop.\n",
    "\n",
    "When you run the next step, the K-means code will produce an animation that steps you through the progress of the algorithm at each iteration. At the end, your figure should look as the one displayed below.\n",
    "\n",
    "![](Figures/kmeans_result.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an example dataset\n",
    "data = loadmat('Data/ex7data2.mat')\n",
    "\n",
    "# Settings for running K-Means\n",
    "K = 3\n",
    "max_iters = 10\n",
    "\n",
    "# For consistency, here we set centroids to specific values\n",
    "# but in practice you want to generate them automatically, such as by\n",
    "# settings them to be random examples (as can be seen in\n",
    "# kMeansInitCentroids).\n",
    "initial_centroids = np.array([[3, 3], [6, 2], [8, 5]])\n",
    "\n",
    "\n",
    "# Run K-Means algorithm. The 'true' at the end tells our function to plot\n",
    "# the progress of K-Means\n",
    "centroids, idx, anim = utils.runkMeans(X, initial_centroids,\n",
    "                                       findClosestCentroids, computeCentroids, max_iters, True);\n",
    "anim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random initialization \n",
    "\n",
    "The initial assignments of centroids for the example dataset in the previous cell were designed so that you will see the same figure as that shown in the cell above. In practice, a good strategy for initializing the centroids is to select random examples from the training set.\n",
    "\n",
    "In this part of the exercise, you should complete the function `kMeansInitCentroids` as to choose the centroids at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7e23ae312d63195881f34432b9f5018c",
     "grade": false,
     "grade_id": "cell-4762eae6ee3a10ed",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def kMeansInitCentroids(X, K):\n",
    "    \"\"\"\n",
    "    This function initializes K centroids that are to be used in K-means on the dataset x.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array_like \n",
    "        The dataset of size (m x n).\n",
    "    \n",
    "    K : int\n",
    "        The number of clusters.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    centroids : array_like\n",
    "        Centroids of the clusters. This is a matrix of size (K x n).\n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    \n",
    "    # You should return this values correctly\n",
    "    centroids = np.zeros((K, n))\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image compression with K-means (vector quantization)\n",
    "\n",
    "Please read ESL section 14.3.9 here, for a detailed discussion.\n",
    "\n",
    "In this exercise, you will apply K-means to image compression. We will use the image below as an example (property of Frank Wouters).\n",
    "\n",
    "![](Data/bird_small.png)\n",
    "\n",
    "In a straightforward 24-bit color representation of an image, each pixel is represented as three 8-bit unsigned integers (ranging from 0 to 255) that specify the red, green and blue intensity values. This encoding is often referred to as the RGB encoding. Our image contains thousands of colors, and in this part of the exercise, you will reduce the number of colors to 16 colors.\n",
    "\n",
    "By making this reduction, it is possible to represent (compress) the photo in an efficient way. Specifically, you only need to store the RGB values of the 16 selected colors, and for each pixel in the image you now need to only store the index of the color at that location (where only 4 bits are necessary to represent 16 possibilities).\n",
    "\n",
    "In this exercise, you will use the K-means algorithm to select the 16 colors that will be used to represent the compressed image. Concretely, you will treat every pixel in the original image as a data example and use the K-means algorithm to find the 16 colors that best group (cluster) the pixels in the 3-dimensional RGB space. Once you have computed the cluster centroids on the image, you will then use the 16 colors to replace the pixels in the original image.\n",
    "\n",
    "#### K-means on pixels\n",
    "\n",
    "In python, images can be read in as follows:\n",
    "\n",
    "```python\n",
    "# Load 128x128 color image (bird_small.png)\n",
    "img = mpl.image.imread(os.path.join('Data', 'bird_small.png'))\n",
    "```\n",
    "This creates a three-dimensional matrix `A` whose first two indices identify a pixel position and whose last index represents red, green, or blue. For example, A[50, 33, 2] gives the blue intensity of the pixel at row 51 and column 34.\n",
    "\n",
    "The code in the following cell first loads the image, and then reshapes it to create an m x 3 matrix of pixel colors (where m = 16384 = 128 x 128), and calls your K-means function on it.\n",
    "\n",
    "After finding the top K = 16 colors to represent the image, you can now assign each pixel position to its closest centroid using the `findClosestCentroids` function. This allows you to represent the original image using the centroid assignments of each pixel. Notice that you have significantly reduced the number of bits that are required to describe the image. The original image required 24 bits for each one of the 128 x 128 pixel locations, resulting in total size of 128 x 128 x 24 = 393,216 bits. The new representation requires some overhead storage in form of a dictionary of 16 colors, each of which require 24 bits, but the image itself then only requires 4 bits per pixel location. The final number of bits used is therefore 16 x 24 + 128 x 128 x 4 = 65,920 bits, which corresponds to compressing the original image by about a factor of 6.\n",
    "\n",
    "Finally, you can view the effects of the compression by reconstructing the image based only on the centroid assignments. Specifically, you can replace each pixel location with the mean of the centroid assigned to it. The figure below shows the reconstruction we obtained. \n",
    "\n",
    "![](Figures/bird_compression.png)\n",
    "\n",
    "Even though the resulting image retains most of the characteristics of the original, we also see some compression artifacts.\n",
    "\n",
    "Run the following cell to compute the centroids and the centroid allocation of each pixel in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======= Experiment with these parameters ================\n",
    "# You should try different values for those parameters\n",
    "K = 16\n",
    "max_iters = 10\n",
    "\n",
    "# Load an image of a bird\n",
    "# Change the file name and path to experiment with your own images\n",
    "A = mpl.image.imread('Data/bird_small.png')\n",
    "# ==========================================================\n",
    "\n",
    "# Divide by 255 so that all values are in the range 0 - 1\n",
    "A /= 255\n",
    "\n",
    "# Reshape the image into an Nx3 matrix where N = number of pixels.\n",
    "# Each row will contain the Red, Green and Blue pixel values\n",
    "# This gives us our dataset matrix X that we will use K-Means on.\n",
    "X = A.reshape(-1, 3)\n",
    "\n",
    "# When using K-Means, it is important to randomly initialize centroids\n",
    "# You should complete the code in kMeansInitCentroids above before proceeding\n",
    "initial_centroids = kMeansInitCentroids(X, K)\n",
    "\n",
    "# Run K-Means\n",
    "centroids, idx = utils.runkMeans(X, initial_centroids,\n",
    "                                 findClosestCentroids,\n",
    "                                 computeCentroids,\n",
    "                                 max_iters)\n",
    "\n",
    "# We can now recover the image from the indices (idx) by mapping each pixel\n",
    "# (specified by its index in idx) to the centroid value\n",
    "# Reshape the recovered image into proper dimensions\n",
    "X_recovered = centroids[idx, :].reshape(A.shape)\n",
    "\n",
    "# Display the original image, rescale back by 255\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
    "ax[0].imshow(A*255)\n",
    "ax[0].set_title('Original')\n",
    "ax[0].grid(False)\n",
    "\n",
    "# Display compressed image, rescale back by 255\n",
    "ax[1].imshow(X_recovered*255)\n",
    "ax[1].set_title('Compressed, with %d colors' % K)\n",
    "ax[1].grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional (ungraded) exercise: Use your own image\n",
    "\n",
    "In this exercise, modify the code we have supplied in the previous cell to run on one of your own images. Note that if your image is very large, then K-means can take a long time to run. Therefore, we recommend that you resize your images to\n",
    "manageable sizes before running the code. You can also try to vary $K$ to see the effects on the compression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-medoids\n",
    "\n",
    "K-means uses euclidean distance as a dissimilarity measure. This measure puts more weight on larger distances, making K-means sensible to outliers. K-means also requires qualitative features. These restrictions can be removed by generalizing K-means to employ any dissimilarity measure $D_{ii'}$ by explicit optimization with respect to the cluster prototypes, with the price of potentially making the algorithm slower.\n",
    "\n",
    "K-medoids is one such alternative. which restricts cluster centers to be some observation belonging to the cluster.\n",
    "\n",
    "### Implementing K-medoids\n",
    "\n",
    "1. For a given cluster assignment C find the observation in the cluster minimizing total distance to other points in that cluster:\n",
    "$$\n",
    "i_{k}^{*} = \\underset{i:C(i)=k}{\\arg\\min} \\sum_{C(i')=k} D(x_i, x_i').\n",
    "$$\n",
    "Then $m_k = x_{i_k^*}$, $k=1,2,\\ldots,K$ are the current estimates of the cluster centers.\n",
    "\n",
    "2. Given a current set of cluster centers $\\{m_1,\\ldots,m_k\\}$, minimize the total error by assigning each observation to the closest (current) cluster center:\n",
    "$$\n",
    "C(i) = \\underset{1\\leq k \\leq K}{\\arg\\min} D(x_i, m_k).\n",
    "$$\n",
    "\n",
    "3. Iterate steps 1 and 2 until assignments do not change.\n",
    "\n",
    "Solving step 1 required $O(N_k^2)$ computations for each cluster, making K-medoids more expensive than K-means. Other algorithms have been proposed to find the optimal encoder $C$, such as testing changing center $i_k$ with all observations and keeping the change that produces the greatest reduction of $\\sum_{k=1}^K\\sum_{C(i)=k}d_{ii'_{k}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We being by slightly modifying the `findClosestCentroids` functions to use integer indexes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "86144ae02b8079d370790580d522db3d",
     "grade": false,
     "grade_id": "cell-67044fe6816ba3bf",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def findClosestMedoids(X, medoids):\n",
    "    \"\"\"\n",
    "    Computes the medoid memberships for every example.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array_like\n",
    "        The dataset of size (m, n) where each row is a single example. \n",
    "        That is, we have m examples each of n dimensions.\n",
    "        \n",
    "    medoid : array_like\n",
    "        The indexes of the k medoids of size (K). K is the number\n",
    "        of clusters.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    idx : array_like\n",
    "        A vector of size (m, ) which holds the medoid assignment for each\n",
    "        example (row) in the dataset X.\n",
    "    \n",
    "    Instructions\n",
    "    ------------\n",
    "    Go over every example, find its closest medoid, and store\n",
    "    the index inside `idx` at the appropriate location.\n",
    "    Concretely, idx[i] should contain the index of the medoid\n",
    "    closest to example i. Hence, it should be a value in the \n",
    "    range 0..K-1\n",
    "    \"\"\"\n",
    "    \n",
    "    # You need to return the following variables correctly.\n",
    "    idx = np.zeros(X.shape[0], dtype=int)\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7422170df2ed4f51f27bfe1fe2d312c7",
     "grade": true,
     "grade_id": "cell-1a858ca16879833f",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Load an example dataset that we will be using\n",
    "data = loadmat('Data/ex7data2.mat')\n",
    "X = data['X']\n",
    "\n",
    "# Select an initial set of medoids\n",
    "K = 3\n",
    "initial_medoids = np.array([0, 1, 2])\n",
    "\n",
    "# Find the closest centroids for the examples using the initial_centroids\n",
    "idx = findClosestMedoids(X, initial_medoids)\n",
    "\n",
    "print('Closest medoids for the last 3 examples:')\n",
    "print(idx[-3:])\n",
    "print('(the closest medoids should be 2, 2, 0 respectively)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since only step 1 is different from the K-means algorithm, we need to modify the `computeCentroids` function into the `computeMedoids` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "29c1889f207ceb132d506aad8aa43f42",
     "grade": false,
     "grade_id": "cell-622c5638db19dda1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def computeMedoids(X, idx, K):\n",
    "    \"\"\"\n",
    "    Returns the new centroids indices by computing finding the observation that minimizes\n",
    "    the within cluster scatter.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array_like\n",
    "        The datset where each row is a single data point. That is, it \n",
    "        is a matrix of size (m, n) where there are m datapoints each\n",
    "        having n dimensions. \n",
    "    \n",
    "    idx : array_like \n",
    "        A vector (size m) of centroid assignments (i.e. each entry in range [0 ... K-1])\n",
    "        for each example.\n",
    "    \n",
    "    K : int\n",
    "        Number of clusters\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    centroids : array_like\n",
    "        An array of size (K) where each entry is the index of the observation acting as\n",
    "        cluster center.\n",
    "    \n",
    "    Instructions\n",
    "    ------------\n",
    "    Go over every cluster and find the point that minimizes the cluster scatter.\n",
    "    \"\"\"\n",
    "    # Useful variables\n",
    "    m, n = X.shape\n",
    "    # You need to return the following variables correctly.\n",
    "    medoids = np.zeros(K, dtype=int)\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return medoids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bdc878764fca3afc3f0390ab39b7664f",
     "grade": true,
     "grade_id": "cell-3e55827995fedbb7",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Compute medoids based on the closest medoids found in the previous part.\n",
    "medoids = computeMedoids(X, idx, K)\n",
    "\n",
    "print('Medoids computed after initial finding of closest centroids:')\n",
    "print(medoids)\n",
    "print('\\nThe centroids should be')\n",
    "print('   [ 64  16 231]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-medoids on example dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an example dataset\n",
    "data = loadmat('Data/ex7data2.mat')\n",
    "\n",
    "# Settings for running K-Medoids\n",
    "K = 3\n",
    "max_iters = 10\n",
    "\n",
    "initial_medoids = np.array([0, 1, 100])\n",
    "\n",
    "# Run K-Medoids algorithm. The 'true' at the end tells our function to plot\n",
    "# the progress of K-Medoids\n",
    "centroids, idx, anim = utils.runkMedoids(X, initial_medoids,\n",
    "                                         findClosestMedoids,\n",
    "                                         computeMedoids, max_iters, True);\n",
    "anim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Country Dissimilarities (ESL CH14)\n",
    "\n",
    "This example, taken from Kaufman and Rousseeuw (1990), comes from a\n",
    "study in which political science students were asked to provide pairwise dissimilarity measures for 12 countries: Belgium, Brazil, Chile, Cuba, Egypt,\n",
    "France, India, Israel, United States, Union of Soviet Socialist Republics,\n",
    "Yugoslavia and Zaire. The average dissimilarity scores are given in the table. \n",
    "We applied 3-medoid clustering to these dissimilarities. Note that\n",
    "K-means clustering could not be applied because we have only distances\n",
    "rather than raw observations. The figure shows a two-dimensional multidimensional scaling plot, with\n",
    "the 3-medoid clusters assignments indicated by colors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = pd.read_csv('Data/countries.data', sep='\\s+',\n",
    "                        names=['BEL', 'BRA', 'CHI', 'CUB', 'EGY', 'FRA',\n",
    "                               'IND', 'ISR', 'USA', 'USS', 'YUG', 'ZAI'])\n",
    "countries.index = ['BEL', 'BRA', 'CHI', 'CUB', 'EGY', 'FRA',\n",
    "                   'IND', 'ISR', 'USA', 'USS', 'YUG', 'ZAI']\n",
    "countries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use a dissimilarity matrix directly we need to modify our K-medoids functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5178ce5b0c53d9fcf0fd0a7ad6a1d55d",
     "grade": false,
     "grade_id": "cell-c1593e93c0de85f9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def findClosestMedoidsDiss(D, medoids):\n",
    "    \"\"\"\n",
    "    Computes the medoid memberships for every example.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    D : array_like\n",
    "        A precomputed dissimilarity matrix.\n",
    "        \n",
    "    medoid : array_like\n",
    "        The indexes of the k medoids of size (K). K is the number\n",
    "        of clusters.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    idx : array_like\n",
    "        A vector of size (m, ) which holds the medoid assignment for each\n",
    "        example (row) in the dataset X.\n",
    "    \n",
    "    Instructions\n",
    "    ------------\n",
    "    Go over every example, find its closest medoid, and store\n",
    "    the index inside `idx` at the appropriate location.\n",
    "    Concretely, idx[i] should contain the index of the medoid\n",
    "    closest to example i. Hence, it should be a value in the \n",
    "    range 0..K-1\n",
    "\n",
    "    \"\"\"\n",
    "    # Set K\n",
    "    K = medoids.shape[0]\n",
    "\n",
    "    # You need to return the following variables correctly.\n",
    "    idx = np.zeros(D.shape[0], dtype=int)\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "60835d10764f9e7e77282008fdb9e345",
     "grade": false,
     "grade_id": "cell-c1863df1d983c0d0",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def computeMedoidsDiss(D, idx, K):\n",
    "    \"\"\"\n",
    "    Returns the new centroids indices by computing finding the observation that minimizes\n",
    "    the within cluster scatter.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    D : array_like\n",
    "        The dissimilarity matrix.\n",
    "    \n",
    "    idx : array_like \n",
    "        A vector (size m) of centroid assignments (i.e. each entry in range [0 ... K-1])\n",
    "        for each example.\n",
    "    \n",
    "    K : int\n",
    "        Number of clusters\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    medoids : array_like\n",
    "        An array of size (K) where each entry is the index of the observation acting as\n",
    "        cluster center.\n",
    "    \n",
    "    Instructions\n",
    "    ------------\n",
    "    Go over every cluster and find the point that minimizes the cluster scatter.\n",
    "\n",
    "    \"\"\"\n",
    "    # Useful variables\n",
    "    m = D.shape[0]\n",
    "    # You need to return the following variables correctly.\n",
    "    medoids = np.zeros(K, dtype=int)\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return medoids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run k medoids\n",
    "K = 3\n",
    "max_iters = 100\n",
    "\n",
    "initial_medoids = np.random.choice(range(len(countries)), size=3, replace=False)\n",
    "centroids, idx = utils.runkMedoids(countries.values, initial_medoids,\n",
    "                                   findClosestMedoidsDiss,\n",
    "                                   computeMedoidsDiss, max_iters, False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = pd.DataFrame(data=idx, columns=['Cluster'], index=countries.index)\n",
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To enable visualization, create an MDS scaling plot.\n",
    "from sklearn.manifold import MDS\n",
    "\n",
    "embedding = MDS(n_components=2, dissimilarity='precomputed')\n",
    "D_matrix = countries.values\n",
    "X_t = embedding.fit_transform(D_matrix)\n",
    "\n",
    "plt.scatter(X_t[:,0], X_t[:,1], c=idx)\n",
    "for (x,y), country in zip(X_t, countries.columns):\n",
    "    plt.annotate(country, (x,y));"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
